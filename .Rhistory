# add 3/8/2016 -- look for long gaps.
theLongCatches <- catch[catch$SampleMinutes > fishingGapMinutes,c('SampleDate','StartTime','EndTime','SampleMinutes','TrapStatus','siteID','siteName','trapPositionID','TrapPosition')]      # fishingGapMinutes set in source file -- it's global.
nLongCatches <- nrow(theLongCatches)
if(nLongCatches > 0){
warning("Long gaps were found in the data so the LongGapLoop series will be run.")
# SQL code to find the gaps, and modify trapPositionIDs accordingly.
F.run.sqlFile( ch, "QryFishingGaps.sql", R.FISHGAPMIN=fishingGapMinutes )
# get the updated results
catch2 <- sqlFetch( ch, "TempSumUnmarkedByTrap_Run_Final" )
} else {
# no long gaps to worry about.  so just keep going with the version of catch that we already
# pulled in from the mdb.
}
close(ch)
#   *****
nvisits <- F.buildReportCriteria( site, min.date, max.date )
if( nvisits == 0 ){
warning("Your criteria returned no trapVisit table records.")
return()
}
#   *****
#   Open ODBC channel
db <- get( "db.file", env=.GlobalEnv )
ch <- odbcConnectAccess(db)
#   *****
#   This SQL file develops the hours fished and TempSamplingSummary table
F.run.sqlFile( ch, "QrySamplePeriod.sql", R.TAXON=taxon )
#   *****
#   This SQL generates times when the traps were not fishing
F.run.sqlFile( ch, "QryNotFishing.sql" )
#   *****
#   This SQL generates unmarked fish by run and life stage
F.run.sqlFile( ch, "QryUnmarkedByRunLifestage.sql", R.TAXON=taxon )
#   Now, fetch the result
catch <- sqlFetch( ch, "TempSumUnmarkedByTrap_Run_Final" )
# add 3/8/2016 -- look for long gaps.
theLongCatches <- catch[catch$SampleMinutes > fishingGapMinutes,c('SampleDate','StartTime','EndTime','SampleMinutes','TrapStatus','siteID','siteName','trapPositionID','TrapPosition')]      # fishingGapMinutes set in source file -- it's global.
nLongCatches <- nrow(theLongCatches)
if(nLongCatches > 0){
warning("Long gaps were found in the data so the LongGapLoop series will be run.")
# SQL code to find the gaps, and modify trapPositionIDs accordingly.
F.run.sqlFile( ch, "QryFishingGaps.sql", R.FISHGAPMIN=fishingGapMinutes )
# get the updated results
catch2 <- sqlFetch( ch, "TempSumUnmarkedByTrap_Run_Final" )
} else {
# no long gaps to worry about.  so just keep going with the version of catch that we already
# pulled in from the mdb.
}
dim(catch2)
dim(catch)
head(catch2)
table(catch2$trapPositionID)
catch2[catch2$trapPositionID %in% c(42040,42040.05),]
write.csv(catch2[catch2$trapPositionID %in% c(42040,42040.05),],'C:/Users/jmitchell/Desktop/test.csv')
min.date
max.date
theLongCatches
catch3 <- catch2[catch2$SampleMinutes <= fishingGapMinutes,]
dim(catch2)
dim(catch3)
23141+18
sum(catch3$SampleMinutes) + sum(theLongCatches$SampleMinutes)
sum(catch2$SampleMinutes)
close(ch)
source(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/CAMP_RST",platform,"/R-Interface/source_all_testing.R"))
source(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/source_all_testing.R"))
library(RODBC)
testing <- TRUE                   # points to different output folders.
platform <- 'CAMP_RST20160201'    # points to different platforms
paste(cat('testing == TRUE\n'))
setwd(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/"))
source(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/source_all_testing.R"))
theExcel <- read.csv('theExcel.csv')
theExcel <- theExcel[theExcel$Issues == '',]
rownames(theExcel) <- NULL
by <- 'All'
river <- as.character(droplevels(theExcel[testi,]$streamName))
if(river == ''){
db.file <- db.file1
} else if(river == 'Sacramento River'){
db.file <- db.file2
} else if(river == 'American River'){
db.file <- db.file3
} else if(river == ''){
db.file <- db.file4
} else if(river == 'Feather River'){
db.file <- db.file5
} else if(river == 'Stanislaus River'){
db.file <- db.file6
} else if(river == 'Old American Test'){
db.file <- db.file7
} else if(river == 'Mokelumne River'){
db.file <- db.file8
#   } else if(river == "Knight's Landing"){
#     db.file <- db.file9
} else if(river == "Knight's Landing"){
db.file <- db.fileA
}
if(river != 'Old American Test'){
site         <- theExcel[testi,]$siteID
siteText     <- as.character(droplevels(theExcel[testi,]$Site))
run          <- theExcel[testi,]$RunID
runText      <- as.character(droplevels(theExcel[testi,]$SalmonRun))
min.date     <- as.character(as.Date(theExcel[testi,]$minvisitTime,format = "%m/%d/%Y"))
max.date     <- as.character(as.Date(theExcel[testi,]$maxvisitTime,format = "%m/%d/%Y"))
} else {
#     river        <- 'american'
#     site         <- 57000
#     siteText     <- 'testing'
#     run          <- 4
#     runText      <- 'Winter'
#     min.date     <- "2013-10-01"
#     max.date     <- "2014-09-29"
}
taxon        <- 161980
output.file  <- paste0("..//Outputs//",river,"//a500Run ",testi,"--",by,"_",river,"_",siteText,"_",min.date,"_",max.date)
ci           <- TRUE
output.type  <- "odt"
from         <- "Trent McDonald, Ph.D., WEST Incorporated"
to           <- "Doug Threloff, USFWS CAMP Coordinator"
return.addr  <- "FISH AND WILDLIFE SERVICE!USFWS Caswell State Park Office!1234 Abbey Rd.!Caswell, California  96080!(530) 527-3043, FAX (530) 529-0292"
for(byj in 1:4){
if(byj == 1){
by <- 'day'
} else if(byj == 2){
by <- 'week'
} else if(byj == 3){
by <- 'month'
} else if(byj == 4){
by <- 'year'
}
output.file  <- paste0("..//Outputs//",river,"//Run ",testi,"--",by,"_",river,"_",siteText,"_",min.date,"_",max.date)
F.run.passage      (site, taxon,      min.date, max.date, by=by,     output.file=output.file,         ci=TRUE            )
}
by <- 'All'
output.file  <- paste0("..//Outputs//",river,"//Run ",testi,"--",by,"_",river,"_",siteText,"_",min.date,"_",max.date)
F.lifestage.passage(site, taxon,      min.date, max.date,            output.file,                     ci=TRUE            )
fishingGapMinutes / 24 / 60
close(ch)
fishingGapMinutes / 24 / 60
round(fishingGapMinutes / 24 / 60,2)
library(RODBC)
testing <- TRUE                   # points to different output folders.
platform <- 'CAMP_RST20160201'    # points to different platforms
paste(cat('testing == TRUE\n'))
setwd(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/"))
source(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/source_all_testing.R"))
theExcel <- read.csv('theExcel.csv')
theExcel <- theExcel[theExcel$Issues == '',]
rownames(theExcel) <- NULL
getwd()
#
#   Thim mimics attaching a package.  Eventually this will be replaced when the real package is done.
#
#remove(list=ls())   # erase everything; CAREFUL
.onAttach <- function(){
#   Attach the libraries we need for all or nearly all routines.
library(RODBC)
#   These are other libraries needed by certain routines.  These need to be installed,
#   and will be attached by the routines that need them.
#   library(quantreg)
#   library(splines)
#   library(MASS)
#   library(mvtnorm)  # this is needed in F.bootstrap.passage
#   =================== Global variables
#   Parameter db.file is a string containing the full or relative path to the data base
#db.file <<- "..\\Data\\WorkingVersionCAMP_LAR_16July2013.mdb"  # For trent's testing in 'code' directory
#db.file <<- "..\\Data\\CAMPFeather_NewVersion1July2013.mdb"  # For trent's testing in 'code' directory
#db.file <<- "..\\Data\\connie's caswell stanislaus file after doug adds finaRunIDs  CAMP.mdb"
db.file1 <<- "..\\Data\\TestingDBs\\CAMPBattleClear_29Jan2015\\CAMP.mdb"
db.file2 <<- "..\\Data\\TestingDBs\\CAMP_RBDD_19June20151\\CAMP.mdb"
db.file3 <<- "..\\Data\\TestingDBs\\CAMPAmerican_29Jan2015\\CAMP.mdb"
db.file4 <<- "..\\Data\\TestingDBs\\CAMPCosumnes_25Oct2013_notForAnalyses\\CAMP.mdb"
db.file5 <<- "..\\Data\\TestingDBs\\CAMPFeather_29Jan2015\\CAMP.mdb"
db.file6 <<- "..\\Data\\TestingDBs\\CAMPStanislaus_29Jan2015\\CAMP.mdb"
db.file7 <<- "//lar-file-srv/Data/PSMFC_CampRST/ThePlatform/CAMP_RST20150501/Data/TestingDBs/CAMPAmerican_11Nov2014.mdb"
cat(paste("DB file:", db.file1, "\n"))
cat(paste("DB file:", db.file2, "\n"))
cat(paste("DB file:", db.file3, "\n"))
cat(paste("DB file:", db.file4, "\n"))
cat(paste("DB file:", db.file5, "\n"))
cat(paste("DB file:", db.file6, "\n"))
cat(paste("DB file:", db.file7, "\n"))
#   Parameter table.names is a list containing the mapping of table names in Access to table names in R.
#   This was used to facility painless table name changes in Access.  This should not change unless tables or table names in Access change.
table.names <<- c(trap.visit="TrapVisit",
sites = "Site",
project = "ProjectDescription",
subsites = "SubSite",
catch = "CatchRaw",
release="Release",
mark.applied="MarkApplied",
catch = "CatchRaw",
mark.found="MarkExisting",
trap.visit="TrapVisit",
species.codes="luTaxon",
run.codes ="luRun",
rel.x.target="ReleaseXTargetSite",
LAD = "LengthAtDate",
yes.no.codes="luNoYes",
CAMP.life.stages="luLifeStageCAMP",
life.stages="luLifeStage",
fish.origin="luFishOrigin" )
#   Retreive the YES/NO codes from the luNoYes table.  Just in case they should ever change in the data base
ch <- odbcConnectAccess(db.file7)
luNoYes <- sqlFetch(ch, table.names["yes.no.codes"])
No.code <<- luNoYes$noYesID[ casefold(luNoYes$noYes) == "no" ]
Yes.code <<- luNoYes$noYesID[ casefold(luNoYes$noYes) == "yes" ]
close(ch)
#   Assign sample cut time for batch dates that are missing.
#   If a sample period ends before this time, batch date is the day the period ends.
#   If a sample period ends after this time, batch date is the next day following the end of the sampling period.
samplePeriodCutTime <<- "04:00:00"              # In military time
#   Maximum gap, in hours, that is "okay".  Gaps in trapping smaller than this are ignored.  No catch is imputed for them.
#   Because gam model for imputation predicts an hourly rate, this max gap cannot be < 1 hour
max.ok.gap <<- 2
#   Maximum gap, in minutes, that is NOT okay.  Values of "Not fishing" greater than this value, in minutes,
#   constitute a gap in fishing for which models should not spline.  In other words, they are a large enough
#   break in data to split splines into two separate models.
fishingGapMinutes <<- 10080
#   Write out the memory limit on this machine
cat(paste("Memory limit:", memory.limit(), "Mb \n"))
#   Set time zone. NOTE: all times are assumed to be in this time zone.
#   If not, they may be incorrect.  In any event, all times are forced to this time zone.
time.zone <<- "America/Los_Angeles"
#  *************** NOTE: To do - read the data base and figure out which water shed is being analyzed.  Then,
#  *************** Set the efficiency model to use.
#
#   Specify the capture efficiency model
#eff.model.method <- 3
}
.onAttach()
#   --------------------------------------------------------
#   Source code
source(  "capwords.r"	)
source(	"catch_model.r"	)
source(	"eff_model.r"	)
source(	"est_catch.r"	)
source(	"est_efficiency.r"	)
source(	"est_passage.r"	)
#source(	"find_recaps.r"	)
source(	"get_catch_data.r"	)
source(	"get_all_catch_data.r"	)
source(	"get_release_data.r"	)
#source(	"latex_biweekly_table.r"	)
#source(	"odt_biweekly_table.r"	)
#source(	"latex_passage.r"	)
#source(	"latex_recapSummary.r"	)
source(	"run_passage.r"	)
#source( "annual_passage.r" )
source(	"plot_catch_model.r"	)
source(	"plot_eff_model.r"	)
source(	"plot_passage.r"	)
source(	"release_summary.r"	)
source(	"summarize_releases.r"	)
source( "summarize_fish_visit.r" )
source( "all_catch_table.r" )
#source(	"vec2sqlstr.r"	)
source(	"size_by_date.r"	)
source(	"get_indiv_fish_data.r"	)
source(	"get_indiv_visit_data.r"	)
source(	"length_freq.r"	)
source( "sql_error_check.r" )
#source( "assign_sample_period.r" )
source( "assign_batch_date.r" )
source( "assign_gaplen.r" )
#source( "check_for_missing.r" )
#source( "biweekly_report.r" )
source( "weekly_effort.r" )
#source( "weekly_passage.r" )  # exclude. Same as F.passage with by="week"
source( "lifestage_passage.r" )
source( "bootstrap_passage.r" )
source( "summarize_passage.r" )
source( "summarize_index.r" )
source( "expand_plus_counts.r" )
source( "assign_1dim.r" )
source( "assign_2dim.r" )                             # stuck in an endless loop?  maybe doesnt matter.
source( "get_all_fish_data.r" )
source( "plot_lifestages.r" )
#source( "get_life_stages.r" )
source( "build_report_criteria.r" )
source( "chinook_by_date.r" )
source( "get_by_catch.r" )
source( "by_catch_table.r" )
source( "run_sqlFile.r" )
source( "build_report_criteria_release.r" )
source( "est_catch_trapN.r" )
#
#   Thim mimics attaching a package.  Eventually this will be replaced when the real package is done.
#
.libPaths(.Library)   # check out libpaths -- 01/04/2016.
#remove(list=ls())   # erase everything; CAREFUL
.onAttach <- function(){
#   Attach the libraries we need for all or nearly all routines.
library(RODBC)
#   These are other libraries needed by certain routines.  These need to be installed,
#   and will be attached by the routines that need them.
#   library(quantreg)
#   library(splines)
#   library(MASS)
#   library(mvtnorm)  # this is needed in F.bootstrap.passage
#   =================== Global variables
#   Parameter db.file is a string containing the full or relative path to the data base
#db.file <<- "..\\Data\\WorkingVersionCAMP_LAR_16July2013.mdb"  # For trent's testing in 'code' directory
#db.file <<- "..\\Data\\CAMPFeather_NewVersion1July2013.mdb"  # For trent's testing in 'code' directory
#db.file <<- "..\\Data\\connie's caswell stanislaus file after doug adds finaRunIDs  CAMP.mdb"
db.file  <<- "..\\Data\\CAMP.mdb"    #   THE OFFICIAL DATA BASE
cat(paste("DB file:", db.file ,"\n"))
#   Parameter table.names is a list containing the mapping of table names in Access to table names in R.
#   This was used to facility painless table name changes in Access.  This should not change unless tables or table names in Access change.
table.names <<- c(trap.visit="TrapVisit",
sites = "Site",
project = "ProjectDescription",
subsites = "SubSite",
catch = "CatchRaw",
release="Release",
mark.applied="MarkApplied",
catch = "CatchRaw",
mark.found="MarkExisting",
trap.visit="TrapVisit",
species.codes="luTaxon",
run.codes ="luRun",
rel.x.target="ReleaseXTargetSite",
LAD = "LengthAtDate",
yes.no.codes="luNoYes",
CAMP.life.stages="luLifeStageCAMP",
life.stages="luLifeStage",
fish.origin="luFishOrigin" )
#   Retreive the YES/NO codes from the luNoYes table.  Just in case they should ever change in the data base
#     setwd('C:/Users/jmitchell/Desktop/CAMP_RST-master/Jason')
#     ch <- odbcConnectAccess('CAMPStanislaus_16Sept2013.mdb')
ch <- odbcConnectAccess(db.file)
luNoYes <- sqlFetch(ch, table.names["yes.no.codes"])
No.code <<- luNoYes$noYesID[ casefold(luNoYes$noYes) == "no" ]
Yes.code <<- luNoYes$noYesID[ casefold(luNoYes$noYes) == "yes" ]
close(ch)
#   Assign sample cut time for batch dates that are missing.
#   If a sample period ends before this time, batch date is the day the period ends.
#   If a sample period ends after this time, batch date is the next day following the end of the sampling period.
samplePeriodCutTime <<- "04:00:00"              # In military time
#   Maximum gap, in hours, that is "okay".  Gaps in trapping smaller than this are ignored.  No catch is imputed for them.
#   Because gam model for imputation predicts an hourly rate, this max gap cannot be < 1 hour
max.ok.gap <<- 2
#   Maximum gap, in minutes, that is NOT okay.  Values of "Not fishing" greater than this value, in minutes,
#   constitute a gap in fishing for which models should not spline.  In other words, they are a large enough
#   break in data to split splines into two separate models.
fishingGapMinutes <<- 10080
#   Write out the memory limit on this machine
cat(paste("Memory limit:", memory.limit(), "Mb \n"))
#   Set time zone. NOTE: all times are assumed to be in this time zone.
#   If not, they may be incorrect.  In any event, all times are forced to this time zone.
time.zone <<- "America/Los_Angeles"
#  *************** NOTE: To do - read the data base and figure out which water shed is being analyzed.  Then,
#  *************** Set the efficiency model to use.
#
#   Specify the capture efficiency model
#eff.model.method <- 3
}
.onAttach()
#   --------------------------------------------------------
#   Source code
source(	"capwords.r"	)
source(	"catch_model.r"	)
source(	"eff_model.r"	)
source(	"est_catch.r"	)
source(	"est_efficiency.r"	)
source(	"est_passage.r"	)
#source(	"find_recaps.r"	)
source(	"get_catch_data.r"	)
source(	"get_all_catch_data.r"	)
source(	"get_release_data.r"	)
#source(	"latex_biweekly_table.r"	)
#source(	"odt_biweekly_table.r"	)
#source(	"latex_passage.r"	)
#source(	"latex_recapSummary.r"	)
source( "run_passage.r" )
source(	"passage.r"	)
source( "annual_passage.r" )
source(	"plot_catch_model.r"	)
source(	"plot_eff_model.r"	)
source(	"plot_passage.r"	)
source(	"release_summary.r"	)
source( "run_passage.r" )
source(	"summarize_releases.r"	)
source( "summarize_fish_visit.r" )
source( "all_catch_table.r" )
#source(	"vec2sqlstr.r"	)
source(	"size_by_date.r"	)
source(	"get_indiv_fish_data.r"	)
source(	"get_indiv_visit_data.r"	)
source(	"length_freq.r"	)
source( "sql_error_check.r" )
#source( "assign_sample_period.r" )
source( "assign_batch_date.r" )
source( "assign_gaplen.r" )
#source( "check_for_missing.r" )
#source( "biweekly_report.r" )
source( "weekly_effort.r" )
#source( "weekly_passage.r" )  # exclude. Same as F.passage with by="week"
source( "lifestage_passage.r" )
source( "bootstrap_passage.r" )
source( "summarize_passage.r" )
source( "summarize_index.r" )
source( "expand_plus_counts.r" )
source( "assign_1dim.r" )
source( "assign_2dim.r" )                             # stuck in an endless loop?  maybe doesnt matter.
source( "get_all_fish_data.r" )
source( "plot_lifestages.r" )
#source( "get_life_stages.r" )
source( "build_report_criteria.r" )
source( "chinook_by_date.r" )
source( "get_by_catch.r" )
source( "by_catch_table.r" )
source( "run_sqlFile.r" )
source( "build_report_criteria_release.r" )
source( "plot_spline.R" )
source( "accounting.r" )
source( "getTheData.r" )
source( "max_buff_days.r ")
source( "chuck_zeros.r" )
source( "est_catch_trapN.r" )
by <- 'All'
river <- as.character(droplevels(theExcel[testi,]$streamName))
if(river == ''){
db.file <- db.file1
} else if(river == 'Sacramento River'){
db.file <- db.file2
} else if(river == 'American River'){
db.file <- db.file3
} else if(river == ''){
db.file <- db.file4
} else if(river == 'Feather River'){
db.file <- db.file5
} else if(river == 'Stanislaus River'){
db.file <- db.file6
} else if(river == 'Old American Test'){
db.file <- db.file7
} else if(river == 'Mokelumne River'){
db.file <- db.file8
#   } else if(river == "Knight's Landing"){
#     db.file <- db.file9
} else if(river == "Knight's Landing"){
db.file <- db.fileA
}
if(river != 'Old American Test'){
site         <- theExcel[testi,]$siteID
siteText     <- as.character(droplevels(theExcel[testi,]$Site))
run          <- theExcel[testi,]$RunID
runText      <- as.character(droplevels(theExcel[testi,]$SalmonRun))
min.date     <- as.character(as.Date(theExcel[testi,]$minvisitTime,format = "%m/%d/%Y"))
max.date     <- as.character(as.Date(theExcel[testi,]$maxvisitTime,format = "%m/%d/%Y"))
} else {
#     river        <- 'american'
#     site         <- 57000
#     siteText     <- 'testing'
#     run          <- 4
#     runText      <- 'Winter'
#     min.date     <- "2013-10-01"
#     max.date     <- "2014-09-29"
}
taxon        <- 161980
output.file  <- paste0("..//Outputs//",river,"//a500Run ",testi,"--",by,"_",river,"_",siteText,"_",min.date,"_",max.date)
ci           <- TRUE
output.type  <- "odt"
from         <- "Trent McDonald, Ph.D., WEST Incorporated"
to           <- "Doug Threloff, USFWS CAMP Coordinator"
return.addr  <- "FISH AND WILDLIFE SERVICE!USFWS Caswell State Park Office!1234 Abbey Rd.!Caswell, California  96080!(530) 527-3043, FAX (530) 529-0292"
for(byj in 1:4){
if(byj == 1){
by <- 'day'
} else if(byj == 2){
by <- 'week'
} else if(byj == 3){
by <- 'month'
} else if(byj == 4){
by <- 'year'
}
output.file  <- paste0("..//Outputs//",river,"//Run ",testi,"--",by,"_",river,"_",siteText,"_",min.date,"_",max.date)
F.run.passage      (site, taxon,      min.date, max.date, by=by,     output.file=output.file,         ci=TRUE            )
}
require("RODBC")
require("mvtnorm")
# install RODBC
# install mvtnorm
install.packages(c("RODBC","mvtnorm"))
require("RODBC")
require("mvtnorm")
library(RODBC)
testing <- TRUE                   # points to different output folders.
platform <- 'CAMP_RST20160201'    # points to different platforms
paste(cat('testing == TRUE\n'))
setwd(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/"))
source(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/source_all_testing.R"))
theExcel <- read.csv('theExcel.csv')
theExcel <- theExcel[theExcel$Issues == '',]
rownames(theExcel) <- NULL
install.packages(c("RODBC", "mvtnorm"))
# install RODBC
# install mvtnorm
install.packages(c("RODBC","mvtnorm"))
require("RODBC")
require("mvtnorm")
library(RODBC)
testing <- TRUE                   # points to different output folders.
platform <- 'CAMP_RST20160201'    # points to different platforms
paste(cat('testing == TRUE\n'))
setwd(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/"))
source(paste0("\\\\LAR-FILE-SRV/Data/PSMFC_CampRST/ThePlatform/",platform,"/R-Interface/source_all_testing.R"))
theExcel <- read.csv('theExcel.csv')
theExcel <- theExcel[theExcel$Issues == '',]
rownames(theExcel) <- NULL
install.packages(c("RODBC", "mvtnorm"))
